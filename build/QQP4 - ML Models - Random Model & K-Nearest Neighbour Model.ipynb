{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Loading the required libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from collections import Counter\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "# Some helper functions:\n",
    "def get_shape(seq):\n",
    "    if type(seq) == type([]):\n",
    "        print(\"The shape of data is:\", len(seq),\",\",len(seq[0]))\n",
    "    else:\n",
    "        print(\"The shape of data is:\", seq.shape)\n",
    "    return\n",
    "\n",
    "def time_taken(start):\n",
    "    print(\"\\nRuntime:\", round(time()-start, 2), \"seconds\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Machine Learning Models\n",
    "We will apply two ML Algorithms and generate 2 models:\n",
    "1. Random Modelling Algorithm on a Sample of Quora Question Pairs Data.\n",
    "2. K-Nearest Neighbour Algorithm on a Sample of Quora Question Pairs Data.\n",
    "\n",
    "## 4.1 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "The shape of data is: (25000, 797)\n",
      "\n",
      "Runtime: 7.92 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>ctc_min</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>...</th>\n",
       "      <th>374_y</th>\n",
       "      <th>375_y</th>\n",
       "      <th>376_y</th>\n",
       "      <th>377_y</th>\n",
       "      <th>378_y</th>\n",
       "      <th>379_y</th>\n",
       "      <th>380_y</th>\n",
       "      <th>381_y</th>\n",
       "      <th>382_y</th>\n",
       "      <th>383_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.833319</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.916659</td>\n",
       "      <td>0.785709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.188503</td>\n",
       "      <td>33.233713</td>\n",
       "      <td>6.971700</td>\n",
       "      <td>-14.820828</td>\n",
       "      <td>15.534945</td>\n",
       "      <td>8.205955</td>\n",
       "      <td>-25.256606</td>\n",
       "      <td>1.552828</td>\n",
       "      <td>1.651827</td>\n",
       "      <td>0.267462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.799984</td>\n",
       "      <td>0.399996</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.599988</td>\n",
       "      <td>0.699993</td>\n",
       "      <td>0.466664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.432317</td>\n",
       "      <td>-4.367793</td>\n",
       "      <td>41.101273</td>\n",
       "      <td>-0.930737</td>\n",
       "      <td>-15.686246</td>\n",
       "      <td>-7.275999</td>\n",
       "      <td>2.756560</td>\n",
       "      <td>-7.351970</td>\n",
       "      <td>3.103773</td>\n",
       "      <td>0.440425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.333328</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.249997</td>\n",
       "      <td>0.399996</td>\n",
       "      <td>0.285712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.264448</td>\n",
       "      <td>-2.244750</td>\n",
       "      <td>11.084606</td>\n",
       "      <td>-16.741266</td>\n",
       "      <td>14.854023</td>\n",
       "      <td>15.726977</td>\n",
       "      <td>-1.298039</td>\n",
       "      <td>14.340431</td>\n",
       "      <td>11.669012</td>\n",
       "      <td>10.423255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.488654</td>\n",
       "      <td>3.906499</td>\n",
       "      <td>13.387563</td>\n",
       "      <td>-6.640244</td>\n",
       "      <td>6.378005</td>\n",
       "      <td>6.028185</td>\n",
       "      <td>2.511873</td>\n",
       "      <td>-3.830347</td>\n",
       "      <td>5.421078</td>\n",
       "      <td>6.161891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.199998</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.571420</td>\n",
       "      <td>0.307690</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.440844</td>\n",
       "      <td>11.887040</td>\n",
       "      <td>8.019029</td>\n",
       "      <td>-15.028031</td>\n",
       "      <td>8.280575</td>\n",
       "      <td>1.703147</td>\n",
       "      <td>-6.503707</td>\n",
       "      <td>11.263387</td>\n",
       "      <td>11.556818</td>\n",
       "      <td>2.500520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 797 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id  is_duplicate   cwc_min   cwc_max   csc_min   csc_max  \\\n",
       "0           0   0             0  0.999980  0.833319  0.999983  0.999983   \n",
       "1           1   1             0  0.799984  0.399996  0.749981  0.599988   \n",
       "2           2   2             0  0.399992  0.333328  0.399992  0.249997   \n",
       "3           3   3             0  0.000000  0.000000  0.000000  0.000000   \n",
       "4           4   4             0  0.399992  0.199998  0.999950  0.666644   \n",
       "\n",
       "    ctc_min   ctc_max  last_word_eq    ...          374_y      375_y  \\\n",
       "0  0.916659  0.785709           0.0    ...      16.188503  33.233713   \n",
       "1  0.699993  0.466664           0.0    ...      -4.432317  -4.367793   \n",
       "2  0.399996  0.285712           0.0    ...       8.264448  -2.244750   \n",
       "3  0.000000  0.000000           0.0    ...       3.488654   3.906499   \n",
       "4  0.571420  0.307690           0.0    ...      -2.440844  11.887040   \n",
       "\n",
       "       376_y      377_y      378_y      379_y      380_y      381_y  \\\n",
       "0   6.971700 -14.820828  15.534945   8.205955 -25.256606   1.552828   \n",
       "1  41.101273  -0.930737 -15.686246  -7.275999   2.756560  -7.351970   \n",
       "2  11.084606 -16.741266  14.854023  15.726977  -1.298039  14.340431   \n",
       "3  13.387563  -6.640244   6.378005   6.028185   2.511873  -3.830347   \n",
       "4   8.019029 -15.028031   8.280575   1.703147  -6.503707  11.263387   \n",
       "\n",
       "       382_y      383_y  \n",
       "0   1.651827   0.267462  \n",
       "1   3.103773   0.440425  \n",
       "2  11.669012  10.423255  \n",
       "3   5.421078   6.161891  \n",
       "4  11.556818   2.500520  \n",
       "\n",
       "[5 rows x 797 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = time()\n",
    "# Load only a sample of the final features data:\n",
    "quora_df = pd.read_csv('./final_features_100k.csv', nrows=25000)\n",
    "\n",
    "# Data Info:\n",
    "print(type(quora_df))\n",
    "get_shape(quora_df)\n",
    "time_taken(st)\n",
    "quora_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>ctc_min</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>...</th>\n",
       "      <th>374_y</th>\n",
       "      <th>375_y</th>\n",
       "      <th>376_y</th>\n",
       "      <th>377_y</th>\n",
       "      <th>378_y</th>\n",
       "      <th>379_y</th>\n",
       "      <th>380_y</th>\n",
       "      <th>381_y</th>\n",
       "      <th>382_y</th>\n",
       "      <th>383_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>24995</td>\n",
       "      <td>24995</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.499988</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.571420</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.164913</td>\n",
       "      <td>23.039302</td>\n",
       "      <td>5.387981</td>\n",
       "      <td>0.075391</td>\n",
       "      <td>5.558455</td>\n",
       "      <td>-2.099565</td>\n",
       "      <td>-2.638070</td>\n",
       "      <td>-7.089084</td>\n",
       "      <td>-5.356800</td>\n",
       "      <td>10.654196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>24996</td>\n",
       "      <td>24996</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.799984</td>\n",
       "      <td>0.799984</td>\n",
       "      <td>0.749991</td>\n",
       "      <td>0.749991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.033602</td>\n",
       "      <td>15.150122</td>\n",
       "      <td>6.668069</td>\n",
       "      <td>-2.651128</td>\n",
       "      <td>13.367594</td>\n",
       "      <td>5.552914</td>\n",
       "      <td>-11.574743</td>\n",
       "      <td>0.417787</td>\n",
       "      <td>7.598512</td>\n",
       "      <td>1.206826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>24997</td>\n",
       "      <td>24997</td>\n",
       "      <td>0</td>\n",
       "      <td>0.599988</td>\n",
       "      <td>0.428565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.299997</td>\n",
       "      <td>0.299997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.951352</td>\n",
       "      <td>0.726570</td>\n",
       "      <td>5.036810</td>\n",
       "      <td>-6.168143</td>\n",
       "      <td>-8.204763</td>\n",
       "      <td>6.214043</td>\n",
       "      <td>-4.996871</td>\n",
       "      <td>5.989402</td>\n",
       "      <td>12.023006</td>\n",
       "      <td>-12.509062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>24998</td>\n",
       "      <td>24998</td>\n",
       "      <td>1</td>\n",
       "      <td>0.499988</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.499975</td>\n",
       "      <td>0.199996</td>\n",
       "      <td>0.499992</td>\n",
       "      <td>0.299997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.845878</td>\n",
       "      <td>24.765205</td>\n",
       "      <td>5.713985</td>\n",
       "      <td>-8.277235</td>\n",
       "      <td>14.101339</td>\n",
       "      <td>3.312259</td>\n",
       "      <td>-7.348258</td>\n",
       "      <td>4.242257</td>\n",
       "      <td>22.634850</td>\n",
       "      <td>0.331471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>24999</td>\n",
       "      <td>24999</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.857131</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.818174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.514413</td>\n",
       "      <td>2.062461</td>\n",
       "      <td>2.855732</td>\n",
       "      <td>-5.723221</td>\n",
       "      <td>-0.315934</td>\n",
       "      <td>8.459152</td>\n",
       "      <td>-7.826173</td>\n",
       "      <td>3.594841</td>\n",
       "      <td>1.028608</td>\n",
       "      <td>1.307878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 797 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     id  is_duplicate   cwc_min   cwc_max   csc_min  \\\n",
       "24995       24995  24995             0  0.999950  0.666644  0.999950   \n",
       "24996       24996  24996             0  0.666644  0.666644  0.799984   \n",
       "24997       24997  24997             0  0.599988  0.428565  0.000000   \n",
       "24998       24998  24998             1  0.499988  0.399992  0.499975   \n",
       "24999       24999  24999             0  0.999967  0.749981  0.999983   \n",
       "\n",
       "        csc_max   ctc_min   ctc_max  last_word_eq    ...         374_y  \\\n",
       "24995  0.499988  0.999975  0.571420           1.0    ...      7.164913   \n",
       "24996  0.799984  0.749991  0.749991           0.0    ...      2.033602   \n",
       "24997  0.000000  0.299997  0.299997           0.0    ...     -2.951352   \n",
       "24998  0.199996  0.499992  0.299997           0.0    ...      9.845878   \n",
       "24999  0.857131  0.999989  0.818174           0.0    ...      5.514413   \n",
       "\n",
       "           375_y     376_y     377_y      378_y     379_y      380_y  \\\n",
       "24995  23.039302  5.387981  0.075391   5.558455 -2.099565  -2.638070   \n",
       "24996  15.150122  6.668069 -2.651128  13.367594  5.552914 -11.574743   \n",
       "24997   0.726570  5.036810 -6.168143  -8.204763  6.214043  -4.996871   \n",
       "24998  24.765205  5.713985 -8.277235  14.101339  3.312259  -7.348258   \n",
       "24999   2.062461  2.855732 -5.723221  -0.315934  8.459152  -7.826173   \n",
       "\n",
       "          381_y      382_y      383_y  \n",
       "24995 -7.089084  -5.356800  10.654196  \n",
       "24996  0.417787   7.598512   1.206826  \n",
       "24997  5.989402  12.023006 -12.509062  \n",
       "24998  4.242257  22.634850   0.331471  \n",
       "24999  3.594841   1.028608   1.307878  \n",
       "\n",
       "[5 rows x 797 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quora_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>ctc_min</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>first_word_eq</th>\n",
       "      <th>abs_len_diff</th>\n",
       "      <th>mean_len</th>\n",
       "      <th>...</th>\n",
       "      <th>374_y</th>\n",
       "      <th>375_y</th>\n",
       "      <th>376_y</th>\n",
       "      <th>377_y</th>\n",
       "      <th>378_y</th>\n",
       "      <th>379_y</th>\n",
       "      <th>380_y</th>\n",
       "      <th>381_y</th>\n",
       "      <th>382_y</th>\n",
       "      <th>383_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.833319</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.916659</td>\n",
       "      <td>0.785709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.188503</td>\n",
       "      <td>33.233713</td>\n",
       "      <td>6.971700</td>\n",
       "      <td>-14.820828</td>\n",
       "      <td>15.534945</td>\n",
       "      <td>8.205955</td>\n",
       "      <td>-25.256606</td>\n",
       "      <td>1.552828</td>\n",
       "      <td>1.651827</td>\n",
       "      <td>0.267462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.799984</td>\n",
       "      <td>0.399996</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.599988</td>\n",
       "      <td>0.699993</td>\n",
       "      <td>0.466664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.432317</td>\n",
       "      <td>-4.367793</td>\n",
       "      <td>41.101273</td>\n",
       "      <td>-0.930737</td>\n",
       "      <td>-15.686246</td>\n",
       "      <td>-7.275999</td>\n",
       "      <td>2.756560</td>\n",
       "      <td>-7.351970</td>\n",
       "      <td>3.103773</td>\n",
       "      <td>0.440425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.333328</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.249997</td>\n",
       "      <td>0.399996</td>\n",
       "      <td>0.285712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.264448</td>\n",
       "      <td>-2.244750</td>\n",
       "      <td>11.084606</td>\n",
       "      <td>-16.741266</td>\n",
       "      <td>14.854023</td>\n",
       "      <td>15.726977</td>\n",
       "      <td>-1.298039</td>\n",
       "      <td>14.340431</td>\n",
       "      <td>11.669012</td>\n",
       "      <td>10.423255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.488654</td>\n",
       "      <td>3.906499</td>\n",
       "      <td>13.387563</td>\n",
       "      <td>-6.640244</td>\n",
       "      <td>6.378005</td>\n",
       "      <td>6.028185</td>\n",
       "      <td>2.511873</td>\n",
       "      <td>-3.830347</td>\n",
       "      <td>5.421078</td>\n",
       "      <td>6.161891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.199998</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.571420</td>\n",
       "      <td>0.307690</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.440844</td>\n",
       "      <td>11.887040</td>\n",
       "      <td>8.019029</td>\n",
       "      <td>-15.028031</td>\n",
       "      <td>8.280575</td>\n",
       "      <td>1.703147</td>\n",
       "      <td>-6.503707</td>\n",
       "      <td>11.263387</td>\n",
       "      <td>11.556818</td>\n",
       "      <td>2.500520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 794 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    cwc_min   cwc_max   csc_min   csc_max   ctc_min   ctc_max  last_word_eq  \\\n",
       "0  0.999980  0.833319  0.999983  0.999983  0.916659  0.785709           0.0   \n",
       "1  0.799984  0.399996  0.749981  0.599988  0.699993  0.466664           0.0   \n",
       "2  0.399992  0.333328  0.399992  0.249997  0.399996  0.285712           0.0   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000           0.0   \n",
       "4  0.399992  0.199998  0.999950  0.666644  0.571420  0.307690           0.0   \n",
       "\n",
       "   first_word_eq  abs_len_diff  mean_len    ...          374_y      375_y  \\\n",
       "0            1.0           0.0      13.0    ...      16.188503  33.233713   \n",
       "1            1.0           0.0      12.5    ...      -4.432317  -4.367793   \n",
       "2            1.0           0.0      12.0    ...       8.264448  -2.244750   \n",
       "3            0.0           0.0      12.0    ...       3.488654   3.906499   \n",
       "4            1.0           0.0      10.0    ...      -2.440844  11.887040   \n",
       "\n",
       "       376_y      377_y      378_y      379_y      380_y      381_y  \\\n",
       "0   6.971700 -14.820828  15.534945   8.205955 -25.256606   1.552828   \n",
       "1  41.101273  -0.930737 -15.686246  -7.275999   2.756560  -7.351970   \n",
       "2  11.084606 -16.741266  14.854023  15.726977  -1.298039  14.340431   \n",
       "3  13.387563  -6.640244   6.378005   6.028185   2.511873  -3.830347   \n",
       "4   8.019029 -15.028031   8.280575   1.703147  -6.503707  11.263387   \n",
       "\n",
       "       382_y      383_y  \n",
       "0   1.651827   0.267462  \n",
       "1   3.103773   0.440425  \n",
       "2  11.669012  10.423255  \n",
       "3   5.421078   6.161891  \n",
       "4  11.556818   2.500520  \n",
       "\n",
       "[5 rows x 794 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will drop some useless features:\n",
    "y_class = quora_df['is_duplicate']\n",
    "quora_df.drop(['id', 'is_duplicate', 'Unnamed: 0'], axis=1, inplace=True)\n",
    "quora_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: is_duplicate, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have our class variable as:\n",
    "y_class.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Converting strings to numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwc_min, cwc_max, csc_min, csc_max, ctc_min, ctc_max, last_word_eq, first_word_eq, abs_len_diff, mean_len, token_set_ratio, token_sort_ratio, fuzz_ratio, fuzz_parital_ratio, longest_substr_ratio, freq_qid1, freq_qid2, q1len, q2len, q1_n_words, q2_n_words, word_Common, word_Total, word_share, freq_q1+q2, freq_q1-q2, 0_x, 1_x, 2_x, 3_x, 4_x, 5_x, 6_x, 7_x, 8_x, 9_x, 10_x, 11_x, 12_x, 13_x, 14_x, 15_x, 16_x, 17_x, 18_x, 19_x, 20_x, 21_x, 22_x, 23_x, 24_x, 25_x, 26_x, 27_x, 28_x, 29_x, 30_x, 31_x, 32_x, 33_x, 34_x, 35_x, 36_x, 37_x, 38_x, 39_x, 40_x, 41_x, 42_x, 43_x, 44_x, 45_x, 46_x, 47_x, 48_x, 49_x, 50_x, 51_x, 52_x, 53_x, 54_x, 55_x, 56_x, 57_x, 58_x, 59_x, 60_x, 61_x, 62_x, 63_x, 64_x, 65_x, 66_x, 67_x, 68_x, 69_x, 70_x, 71_x, 72_x, 73_x, 74_x, 75_x, 76_x, 77_x, 78_x, 79_x, 80_x, 81_x, 82_x, 83_x, 84_x, 85_x, 86_x, 87_x, 88_x, 89_x, 90_x, 91_x, 92_x, 93_x, 94_x, 95_x, 96_x, 97_x, 98_x, 99_x, 100_x, 101_x, 102_x, 103_x, 104_x, 105_x, 106_x, 107_x, 108_x, 109_x, 110_x, 111_x, 112_x, 113_x, 114_x, 115_x, 116_x, 117_x, 118_x, 119_x, 120_x, 121_x, 122_x, 123_x, 124_x, 125_x, 126_x, 127_x, 128_x, 129_x, 130_x, 131_x, 132_x, 133_x, 134_x, 135_x, 136_x, 137_x, 138_x, 139_x, 140_x, 141_x, 142_x, 143_x, 144_x, 145_x, 146_x, 147_x, 148_x, 149_x, 150_x, 151_x, 152_x, 153_x, 154_x, 155_x, 156_x, 157_x, 158_x, 159_x, 160_x, 161_x, 162_x, 163_x, 164_x, 165_x, 166_x, 167_x, 168_x, 169_x, 170_x, 171_x, 172_x, 173_x, 174_x, 175_x, 176_x, 177_x, 178_x, 179_x, 180_x, 181_x, 182_x, 183_x, 184_x, 185_x, 186_x, 187_x, 188_x, 189_x, 190_x, 191_x, 192_x, 193_x, 194_x, 195_x, 196_x, 197_x, 198_x, 199_x, 200_x, 201_x, 202_x, 203_x, 204_x, 205_x, 206_x, 207_x, 208_x, 209_x, 210_x, 211_x, 212_x, 213_x, 214_x, 215_x, 216_x, 217_x, 218_x, 219_x, 220_x, 221_x, 222_x, 223_x, 224_x, 225_x, 226_x, 227_x, 228_x, 229_x, 230_x, 231_x, 232_x, 233_x, 234_x, 235_x, 236_x, 237_x, 238_x, 239_x, 240_x, 241_x, 242_x, 243_x, 244_x, 245_x, 246_x, 247_x, 248_x, 249_x, 250_x, 251_x, 252_x, 253_x, 254_x, 255_x, 256_x, 257_x, 258_x, 259_x, 260_x, 261_x, 262_x, 263_x, 264_x, 265_x, 266_x, 267_x, 268_x, 269_x, 270_x, 271_x, 272_x, 273_x, 274_x, 275_x, 276_x, 277_x, 278_x, 279_x, 280_x, 281_x, 282_x, 283_x, 284_x, 285_x, 286_x, 287_x, 288_x, 289_x, 290_x, 291_x, 292_x, 293_x, 294_x, 295_x, 296_x, 297_x, 298_x, 299_x, 300_x, 301_x, 302_x, 303_x, 304_x, 305_x, 306_x, 307_x, 308_x, 309_x, 310_x, 311_x, 312_x, 313_x, 314_x, 315_x, 316_x, 317_x, 318_x, 319_x, 320_x, 321_x, 322_x, 323_x, 324_x, 325_x, 326_x, 327_x, 328_x, 329_x, 330_x, 331_x, 332_x, 333_x, 334_x, 335_x, 336_x, 337_x, 338_x, 339_x, 340_x, 341_x, 342_x, 343_x, 344_x, 345_x, 346_x, 347_x, 348_x, 349_x, 350_x, 351_x, 352_x, 353_x, 354_x, 355_x, 356_x, 357_x, 358_x, 359_x, 360_x, 361_x, 362_x, 363_x, 364_x, 365_x, 366_x, 367_x, 368_x, 369_x, 370_x, 371_x, 372_x, 373_x, 374_x, 375_x, 376_x, 377_x, 378_x, 379_x, 380_x, 381_x, 382_x, 383_x, 0_y, 1_y, 2_y, 3_y, 4_y, 5_y, 6_y, 7_y, 8_y, 9_y, 10_y, 11_y, 12_y, 13_y, 14_y, 15_y, 16_y, 17_y, 18_y, 19_y, 20_y, 21_y, 22_y, 23_y, 24_y, 25_y, 26_y, 27_y, 28_y, 29_y, 30_y, 31_y, 32_y, 33_y, 34_y, 35_y, 36_y, 37_y, 38_y, 39_y, 40_y, 41_y, 42_y, 43_y, 44_y, 45_y, 46_y, 47_y, 48_y, 49_y, 50_y, 51_y, 52_y, 53_y, 54_y, 55_y, 56_y, 57_y, 58_y, 59_y, 60_y, 61_y, 62_y, 63_y, 64_y, 65_y, 66_y, 67_y, 68_y, 69_y, 70_y, 71_y, 72_y, 73_y, 74_y, 75_y, 76_y, 77_y, 78_y, 79_y, 80_y, 81_y, 82_y, 83_y, 84_y, 85_y, 86_y, 87_y, 88_y, 89_y, 90_y, 91_y, 92_y, 93_y, 94_y, 95_y, 96_y, 97_y, 98_y, 99_y, 100_y, 101_y, 102_y, 103_y, 104_y, 105_y, 106_y, 107_y, 108_y, 109_y, 110_y, 111_y, 112_y, 113_y, 114_y, 115_y, 116_y, 117_y, 118_y, 119_y, 120_y, 121_y, 122_y, 123_y, 124_y, 125_y, 126_y, 127_y, 128_y, 129_y, 130_y, 131_y, 132_y, 133_y, 134_y, 135_y, 136_y, 137_y, 138_y, 139_y, 140_y, 141_y, 142_y, 143_y, 144_y, 145_y, 146_y, 147_y, 148_y, 149_y, 150_y, 151_y, 152_y, 153_y, 154_y, 155_y, 156_y, 157_y, 158_y, 159_y, 160_y, 161_y, 162_y, 163_y, 164_y, 165_y, 166_y, 167_y, 168_y, 169_y, 170_y, 171_y, 172_y, 173_y, 174_y, 175_y, 176_y, 177_y, 178_y, 179_y, 180_y, 181_y, 182_y, 183_y, 184_y, 185_y, 186_y, 187_y, 188_y, 189_y, 190_y, 191_y, 192_y, 193_y, 194_y, 195_y, 196_y, 197_y, 198_y, 199_y, 200_y, 201_y, 202_y, 203_y, 204_y, 205_y, 206_y, 207_y, 208_y, 209_y, 210_y, 211_y, 212_y, 213_y, 214_y, 215_y, 216_y, 217_y, 218_y, 219_y, 220_y, 221_y, 222_y, 223_y, 224_y, 225_y, 226_y, 227_y, 228_y, 229_y, 230_y, 231_y, 232_y, 233_y, 234_y, 235_y, 236_y, 237_y, 238_y, 239_y, 240_y, 241_y, 242_y, 243_y, 244_y, 245_y, 246_y, 247_y, 248_y, 249_y, 250_y, 251_y, 252_y, 253_y, 254_y, 255_y, 256_y, 257_y, 258_y, 259_y, 260_y, 261_y, 262_y, 263_y, 264_y, 265_y, 266_y, 267_y, 268_y, 269_y, 270_y, 271_y, 272_y, 273_y, 274_y, 275_y, 276_y, 277_y, 278_y, 279_y, 280_y, 281_y, 282_y, 283_y, 284_y, 285_y, 286_y, 287_y, 288_y, 289_y, 290_y, 291_y, 292_y, 293_y, 294_y, 295_y, 296_y, 297_y, 298_y, 299_y, 300_y, 301_y, 302_y, 303_y, 304_y, 305_y, 306_y, 307_y, 308_y, 309_y, 310_y, 311_y, 312_y, 313_y, 314_y, 315_y, 316_y, 317_y, 318_y, 319_y, 320_y, 321_y, 322_y, 323_y, 324_y, 325_y, 326_y, 327_y, 328_y, 329_y, 330_y, 331_y, 332_y, 333_y, 334_y, 335_y, 336_y, 337_y, 338_y, 339_y, 340_y, 341_y, 342_y, 343_y, 344_y, 345_y, 346_y, 347_y, 348_y, 349_y, 350_y, 351_y, 352_y, 353_y, 354_y, 355_y, 356_y, 357_y, 358_y, 359_y, 360_y, 361_y, 362_y, 363_y, 364_y, 365_y, 366_y, 367_y, 368_y, 369_y, 370_y, 371_y, 372_y, 373_y, 374_y, 375_y, 376_y, 377_y, 378_y, 379_y, 380_y, 381_y, 382_y, 383_y, "
     ]
    }
   ],
   "source": [
    "st = time()\n",
    "\n",
    "cols = list(quora_df.columns)\n",
    "for i in cols:\n",
    "    quora_df[i] = quora_df[i].apply(pd.to_numeric)\n",
    "    print(i, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Runtime: 87.6 seconds\n"
     ]
    }
   ],
   "source": [
    "time_taken(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Random Train-Test Split (70:30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points in train data: (17500, 794)\n",
      "Number of data points in test data : (7500, 794)\n"
     ]
    }
   ],
   "source": [
    "# Our class variable is y_class:\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    quora_df, y_class, stratify=y_class, test_size=0.3\n",
    ")\n",
    "\n",
    "print('Number of data points in train data:', X_train.shape)\n",
    "print('Number of data points in test data :', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>ctc_min</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>first_word_eq</th>\n",
       "      <th>abs_len_diff</th>\n",
       "      <th>mean_len</th>\n",
       "      <th>...</th>\n",
       "      <th>374_y</th>\n",
       "      <th>375_y</th>\n",
       "      <th>376_y</th>\n",
       "      <th>377_y</th>\n",
       "      <th>378_y</th>\n",
       "      <th>379_y</th>\n",
       "      <th>380_y</th>\n",
       "      <th>381_y</th>\n",
       "      <th>382_y</th>\n",
       "      <th>383_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14738</th>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.599988</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.714276</td>\n",
       "      <td>0.624992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>...</td>\n",
       "      <td>6.902403</td>\n",
       "      <td>11.493473</td>\n",
       "      <td>5.613606</td>\n",
       "      <td>-4.877812</td>\n",
       "      <td>13.043599</td>\n",
       "      <td>-1.132098</td>\n",
       "      <td>-4.538146</td>\n",
       "      <td>-3.301893</td>\n",
       "      <td>7.908649</td>\n",
       "      <td>-0.451902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22866</th>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.749991</td>\n",
       "      <td>0.749991</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.047488</td>\n",
       "      <td>2.449508</td>\n",
       "      <td>-5.921124</td>\n",
       "      <td>3.526991</td>\n",
       "      <td>10.510124</td>\n",
       "      <td>2.405635</td>\n",
       "      <td>-7.946949</td>\n",
       "      <td>2.644402</td>\n",
       "      <td>4.655404</td>\n",
       "      <td>0.412322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24591</th>\n",
       "      <td>0.499975</td>\n",
       "      <td>0.499975</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.473008</td>\n",
       "      <td>-2.153633</td>\n",
       "      <td>0.372134</td>\n",
       "      <td>3.989067</td>\n",
       "      <td>-2.932041</td>\n",
       "      <td>-1.881972</td>\n",
       "      <td>-0.737835</td>\n",
       "      <td>15.230009</td>\n",
       "      <td>-3.082860</td>\n",
       "      <td>-5.839749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20393</th>\n",
       "      <td>0.499988</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.599988</td>\n",
       "      <td>0.333330</td>\n",
       "      <td>0.555549</td>\n",
       "      <td>0.333331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.791243</td>\n",
       "      <td>13.849159</td>\n",
       "      <td>1.789727</td>\n",
       "      <td>-7.466151</td>\n",
       "      <td>11.052734</td>\n",
       "      <td>5.252021</td>\n",
       "      <td>-16.688383</td>\n",
       "      <td>4.642186</td>\n",
       "      <td>12.817753</td>\n",
       "      <td>5.000661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21438</th>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.599988</td>\n",
       "      <td>0.749991</td>\n",
       "      <td>0.666659</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>...</td>\n",
       "      <td>4.836686</td>\n",
       "      <td>9.265958</td>\n",
       "      <td>4.292188</td>\n",
       "      <td>-4.779898</td>\n",
       "      <td>13.177334</td>\n",
       "      <td>9.700841</td>\n",
       "      <td>-5.874484</td>\n",
       "      <td>5.485653</td>\n",
       "      <td>7.848238</td>\n",
       "      <td>3.065879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 794 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cwc_min   cwc_max   csc_min   csc_max   ctc_min   ctc_max  \\\n",
       "14738  0.749981  0.599988  0.666644  0.666644  0.714276  0.624992   \n",
       "22866  0.749981  0.749981  0.749981  0.749981  0.749991  0.749991   \n",
       "24591  0.499975  0.499975  0.999900  0.999900  0.666644  0.666644   \n",
       "20393  0.499988  0.399992  0.599988  0.333330  0.555549  0.333331   \n",
       "21438  0.749981  0.749981  0.749981  0.599988  0.749991  0.666659   \n",
       "\n",
       "       last_word_eq  first_word_eq  abs_len_diff  mean_len    ...     \\\n",
       "14738           0.0            1.0           0.0       7.5    ...      \n",
       "22866           1.0            1.0           0.0       8.0    ...      \n",
       "24591           1.0            1.0           0.0       3.0    ...      \n",
       "20393           0.0            0.0           0.0      12.0    ...      \n",
       "21438           0.0            1.0           0.0       8.5    ...      \n",
       "\n",
       "          374_y      375_y     376_y     377_y      378_y     379_y  \\\n",
       "14738  6.902403  11.493473  5.613606 -4.877812  13.043599 -1.132098   \n",
       "22866  3.047488   2.449508 -5.921124  3.526991  10.510124  2.405635   \n",
       "24591  1.473008  -2.153633  0.372134  3.989067  -2.932041 -1.881972   \n",
       "20393 -1.791243  13.849159  1.789727 -7.466151  11.052734  5.252021   \n",
       "21438  4.836686   9.265958  4.292188 -4.779898  13.177334  9.700841   \n",
       "\n",
       "           380_y      381_y      382_y     383_y  \n",
       "14738  -4.538146  -3.301893   7.908649 -0.451902  \n",
       "22866  -7.946949   2.644402   4.655404  0.412322  \n",
       "24591  -0.737835  15.230009  -3.082860 -5.839749  \n",
       "20393 -16.688383   4.642186  12.817753  5.000661  \n",
       "21438  -5.874484   5.485653   7.848238  3.065879  \n",
       "\n",
       "[5 rows x 794 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "X_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>ctc_min</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>first_word_eq</th>\n",
       "      <th>abs_len_diff</th>\n",
       "      <th>mean_len</th>\n",
       "      <th>...</th>\n",
       "      <th>374_y</th>\n",
       "      <th>375_y</th>\n",
       "      <th>376_y</th>\n",
       "      <th>377_y</th>\n",
       "      <th>378_y</th>\n",
       "      <th>379_y</th>\n",
       "      <th>380_y</th>\n",
       "      <th>381_y</th>\n",
       "      <th>382_y</th>\n",
       "      <th>383_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18463</th>\n",
       "      <td>0.333322</td>\n",
       "      <td>0.333322</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.714276</td>\n",
       "      <td>0.714276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.600818</td>\n",
       "      <td>8.941236</td>\n",
       "      <td>4.398032</td>\n",
       "      <td>-3.078369</td>\n",
       "      <td>13.817652</td>\n",
       "      <td>7.158675</td>\n",
       "      <td>-11.921655</td>\n",
       "      <td>7.725214</td>\n",
       "      <td>4.514453</td>\n",
       "      <td>-0.636877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4039</th>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>0.874989</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609111</td>\n",
       "      <td>8.085004</td>\n",
       "      <td>7.153885</td>\n",
       "      <td>1.884657</td>\n",
       "      <td>-11.188039</td>\n",
       "      <td>1.521977</td>\n",
       "      <td>0.628922</td>\n",
       "      <td>2.914077</td>\n",
       "      <td>-0.467034</td>\n",
       "      <td>4.371045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9761</th>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.374995</td>\n",
       "      <td>0.714276</td>\n",
       "      <td>0.333331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.810799</td>\n",
       "      <td>1.547490</td>\n",
       "      <td>3.335044</td>\n",
       "      <td>-3.722337</td>\n",
       "      <td>14.990891</td>\n",
       "      <td>8.770758</td>\n",
       "      <td>-7.148856</td>\n",
       "      <td>5.816961</td>\n",
       "      <td>11.357115</td>\n",
       "      <td>5.122552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12817</th>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.499992</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.454541</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.335906</td>\n",
       "      <td>-3.942303</td>\n",
       "      <td>-5.758041</td>\n",
       "      <td>-1.118189</td>\n",
       "      <td>11.369048</td>\n",
       "      <td>-5.043033</td>\n",
       "      <td>-10.508012</td>\n",
       "      <td>-4.148119</td>\n",
       "      <td>-15.397742</td>\n",
       "      <td>1.834784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16519</th>\n",
       "      <td>0.142855</td>\n",
       "      <td>0.076922</td>\n",
       "      <td>0.499988</td>\n",
       "      <td>0.285710</td>\n",
       "      <td>0.272725</td>\n",
       "      <td>0.149999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.623911</td>\n",
       "      <td>-8.571070</td>\n",
       "      <td>3.896555</td>\n",
       "      <td>-2.758711</td>\n",
       "      <td>-9.771975</td>\n",
       "      <td>-3.437023</td>\n",
       "      <td>-14.918431</td>\n",
       "      <td>16.211051</td>\n",
       "      <td>17.536233</td>\n",
       "      <td>-2.204456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 794 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cwc_min   cwc_max   csc_min   csc_max   ctc_min   ctc_max  \\\n",
       "18463  0.333322  0.333322  0.999975  0.999975  0.714276  0.714276   \n",
       "4039   0.999980  0.999980  0.999950  0.666644  0.999986  0.874989   \n",
       "9761   0.666644  0.399992  0.749981  0.374995  0.714276  0.333331   \n",
       "12817  0.999967  0.499992  0.999950  0.399992  0.999980  0.454541   \n",
       "16519  0.142855  0.076922  0.499988  0.285710  0.272725  0.149999   \n",
       "\n",
       "       last_word_eq  first_word_eq  abs_len_diff  mean_len    ...     \\\n",
       "18463           0.0            1.0           0.0       7.0    ...      \n",
       "4039            1.0            1.0           0.0       7.5    ...      \n",
       "9761            0.0            0.0           0.0      11.0    ...      \n",
       "12817           0.0            1.0           0.0       8.0    ...      \n",
       "16519           0.0            0.0           0.0      15.5    ...      \n",
       "\n",
       "          374_y     375_y     376_y     377_y      378_y     379_y      380_y  \\\n",
       "18463  1.600818  8.941236  4.398032 -3.078369  13.817652  7.158675 -11.921655   \n",
       "4039   0.609111  8.085004  7.153885  1.884657 -11.188039  1.521977   0.628922   \n",
       "9761   6.810799  1.547490  3.335044 -3.722337  14.990891  8.770758  -7.148856   \n",
       "12817 -5.335906 -3.942303 -5.758041 -1.118189  11.369048 -5.043033 -10.508012   \n",
       "16519 -6.623911 -8.571070  3.896555 -2.758711  -9.771975 -3.437023 -14.918431   \n",
       "\n",
       "           381_y      382_y     383_y  \n",
       "18463   7.725214   4.514453 -0.636877  \n",
       "4039    2.914077  -0.467034  4.371045  \n",
       "9761    5.816961  11.357115  5.122552  \n",
       "12817  -4.148119 -15.397742  1.834784  \n",
       "16519  16.211051  17.536233 -2.204456  \n",
       "\n",
       "[5 rows x 794 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of data is: (17500,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14738    1\n",
       "22866    0\n",
       "24591    1\n",
       "20393    1\n",
       "21438    0\n",
       "Name: is_duplicate, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_shape(y_train)\n",
    "y_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of data is: (7500,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18463    0\n",
       "4039     1\n",
       "9761     0\n",
       "12817    1\n",
       "16519    0\n",
       "Name: is_duplicate, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_shape(y_test)\n",
    "y_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Distribution of O/P Variable in train data ----------\n",
      "Number of data points that correspond to 'is_duplicate = 0' are: 10986\n",
      "Number of data points that correspond to 'is_duplicate = 1' are: 6514\n",
      "Total Number of points in train: 17500 \n",
      "\n",
      "O/P (or) class-label: 'is_duplicate'\n",
      "is_duplicate = 0: 0.6277714285714285 \n",
      "is_duplicate = 1: 0.3722285714285714\n"
     ]
    }
   ],
   "source": [
    "# Now we will see the distribution of points classwise:\n",
    "print(\"-\"*10, \"Distribution of O/P Variable in train data\", \"-\"*10)\n",
    "tr_disb = Counter(y_train)\n",
    "print(\"Number of data points that correspond to 'is_duplicate = 0' are:\", tr_disb[0])\n",
    "print(\"Number of data points that correspond to 'is_duplicate = 1' are:\", tr_disb[1])\n",
    "tr_len = len(y_train)\n",
    "print(\"Total Number of points in train:\", tr_len, \"\\n\")\n",
    "print(\"O/P (or) class-label: 'is_duplicate'\")\n",
    "print(\"is_duplicate = 0:\", float(tr_disb[0]/tr_len),\n",
    "     \"\\nis_duplicate = 1:\", float(tr_disb[1]/tr_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Building a Random Model\n",
    "We will find the worst case accuracy score using a random model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will create a list which has exactly same size as our test data\n",
    "# and generate a non-uniform random model:\n",
    "random_y = np.random.choice(2, size=len(y_test), \n",
    "                            p = [0.1, 0.9]\n",
    "                           )\n",
    "random_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for Random Model is 39.800000000000004%\n"
     ]
    }
   ],
   "source": [
    "# Now we check the accuracy score:\n",
    "rand_acc = accuracy_score(random_y, y_test, normalize=True) * float(100)\n",
    "print(\"Accuracy Score for Random Model is {}%\".format(rand_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a Random Model, we are getting ~40% Accuracy, i.e., Our Random Model is able to predict whether 2 questions are similar or not, correctly, only 50% of the time. Therefore, this is the worst case Accuracy Score.\n",
    "\n",
    "We want our k-NN to get an Accuracy Score > 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Building k-Nearest Neighbours Model using Simple Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points in train data: (12250, 794)\n",
      "Number of data points in cross validation data : (5250, 794)\n"
     ]
    }
   ],
   "source": [
    "# Split the train data into cross validation train and cross validation test\n",
    "X_tr, X_cv, y_tr, y_cv = train_test_split(\n",
    "    X_train, y_train, stratify=y_train, test_size=0.3\n",
    ")\n",
    "\n",
    "# train and cv data info:\n",
    "print('Number of data points in train data:', X_tr.shape)\n",
    "print('Number of data points in cross validation data :', X_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Distribution of O/P Variable in train data ---------------\n",
      "Number of data points that correspond to 'is_duplicate = 0' are: 7690\n",
      "Number of data points that correspond to 'is_duplicate = 1' are: 4560\n",
      "Total Number of points in train: 12250 \n",
      "\n",
      "O/P (or) class-label: 'is_duplicate'\n",
      "is_duplicate = 0: 0.6277551020408163 \n",
      "is_duplicate = 1: 0.3722448979591837\n"
     ]
    }
   ],
   "source": [
    "# Now we will see the distribution of points classwise:\n",
    "print(\"-\"*15, \"Distribution of O/P Variable in train data\", \"-\"*15)\n",
    "train_tr_disb = Counter(y_tr)\n",
    "print(\"Number of data points that correspond to 'is_duplicate = 0' are:\", \n",
    "      train_tr_disb[0])\n",
    "print(\"Number of data points that correspond to 'is_duplicate = 1' are:\", \n",
    "      train_tr_disb[1])\n",
    "train_tr_len = len(y_tr)\n",
    "print(\"Total Number of points in train:\", train_tr_len, \"\\n\")\n",
    "print(\"O/P (or) class-label: 'is_duplicate'\")\n",
    "print(\"is_duplicate = 0:\", float(train_tr_disb[0]/train_tr_len),\n",
    "     \"\\nis_duplicate = 1:\", float(train_tr_disb[1]/train_tr_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hyper Parameter Selection (or) Selection of Optimal K__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will test K-NN Algorithm for these values of K:\n",
      "\n",
      "1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 "
     ]
    }
   ],
   "source": [
    "# Finding the right k and applying k-NN using simple cross-validation:\n",
    "# Hyper parameter selection:\n",
    "\n",
    "# Creating odd list of K for K-NN:\n",
    "my_list = list(range(0,100))\n",
    "neighbours = list(filter(lambda x: x%2 != 0, my_list))\n",
    "print(\"We will test K-NN Algorithm for these values of K:\\n\")\n",
    "for i in neighbours:\n",
    "    print(i, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation Accuracy for k=1 is 61.96190476190476%\n",
      "\n",
      "Cross Validation Accuracy for k=3 is 63.29523809523809%\n",
      "\n",
      "Cross Validation Accuracy for k=5 is 63.21904761904762%\n",
      "\n",
      "Cross Validation Accuracy for k=7 is 63.847619047619055%\n",
      "\n",
      "Cross Validation Accuracy for k=9 is 64.07619047619048%\n",
      "\n",
      "Cross Validation Accuracy for k=11 is 64.11428571428571%\n",
      "\n",
      "Cross Validation Accuracy for k=13 is 64.32380952380953%\n",
      "\n",
      "Cross Validation Accuracy for k=15 is 64.36190476190477%\n",
      "\n",
      "Cross Validation Accuracy for k=17 is 64.4%\n",
      "\n",
      "Cross Validation Accuracy for k=19 is 64.64761904761905%\n",
      "\n",
      "Cross Validation Accuracy for k=21 is 64.72380952380952%\n",
      "\n",
      "Cross Validation Accuracy for k=23 is 64.47619047619048%\n",
      "\n",
      "Cross Validation Accuracy for k=25 is 64.64761904761905%\n",
      "\n",
      "Cross Validation Accuracy for k=27 is 65.16190476190476%\n",
      "\n",
      "Cross Validation Accuracy for k=29 is 65.18095238095239%\n",
      "\n",
      "Cross Validation Accuracy for k=31 is 64.91428571428571%\n",
      "\n",
      "Cross Validation Accuracy for k=33 is 65.04761904761904%\n",
      "\n",
      "Cross Validation Accuracy for k=35 is 64.62857142857142%\n",
      "\n",
      "Cross Validation Accuracy for k=37 is 64.32380952380953%\n",
      "\n",
      "Cross Validation Accuracy for k=39 is 64.13333333333333%\n",
      "\n",
      "Cross Validation Accuracy for k=41 is 64.22857142857143%\n",
      "\n",
      "Cross Validation Accuracy for k=43 is 64.03809523809524%\n",
      "\n",
      "Cross Validation Accuracy for k=45 is 64.4%\n",
      "\n",
      "Cross Validation Accuracy for k=47 is 64.53333333333333%\n",
      "\n",
      "Cross Validation Accuracy for k=49 is 64.15238095238095%\n",
      "\n",
      "Cross Validation Accuracy for k=51 is 64.51428571428572%\n",
      "\n",
      "Cross Validation Accuracy for k=53 is 64.59047619047618%\n",
      "\n",
      "Cross Validation Accuracy for k=55 is 64.4%\n",
      "\n",
      "Cross Validation Accuracy for k=57 is 64.28571428571429%\n",
      "\n",
      "Cross Validation Accuracy for k=59 is 64.24761904761904%\n",
      "\n",
      "Cross Validation Accuracy for k=61 is 64.28571428571429%\n",
      "\n",
      "Cross Validation Accuracy for k=63 is 64.41904761904762%\n",
      "\n",
      "Cross Validation Accuracy for k=65 is 64.24761904761904%\n",
      "\n",
      "Cross Validation Accuracy for k=67 is 64.03809523809524%\n",
      "\n",
      "Cross Validation Accuracy for k=69 is 64.15238095238095%\n",
      "\n",
      "Cross Validation Accuracy for k=71 is 63.98095238095238%\n",
      "\n",
      "Cross Validation Accuracy for k=73 is 63.866666666666674%\n",
      "\n",
      "Cross Validation Accuracy for k=75 is 63.82857142857142%\n",
      "\n",
      "Cross Validation Accuracy for k=77 is 63.94285714285714%\n",
      "\n",
      "Cross Validation Accuracy for k=79 is 64.05714285714285%\n",
      "\n",
      "Cross Validation Accuracy for k=81 is 64.15238095238095%\n",
      "\n",
      "Cross Validation Accuracy for k=83 is 64.11428571428571%\n",
      "\n",
      "Cross Validation Accuracy for k=85 is 63.714285714285715%\n",
      "\n",
      "Cross Validation Accuracy for k=87 is 63.90476190476191%\n",
      "\n",
      "Cross Validation Accuracy for k=89 is 63.82857142857142%\n",
      "\n",
      "Cross Validation Accuracy for k=91 is 63.67619047619048%\n",
      "\n",
      "Cross Validation Accuracy for k=93 is 63.542857142857144%\n",
      "\n",
      "Cross Validation Accuracy for k=95 is 63.714285714285715%\n",
      "\n",
      "Cross Validation Accuracy for k=97 is 63.61904761904762%\n",
      "\n",
      "Cross Validation Accuracy for k=99 is 63.88571428571429%\n",
      "\n",
      "Runtime: 2152.39 seconds\n"
     ]
    }
   ],
   "source": [
    "# Now we have all the odd numbers, we can now apply the sklearn\n",
    "# implementation of KNN to know the similarity/polarity between two questions:\n",
    "\n",
    "st = time()\n",
    "\n",
    "# Code for hyper parameter selection:\n",
    "for k in neighbours:\n",
    "    \n",
    "    # Configured parameters are:-\n",
    "    #\n",
    "    # 1. algorithm = 'auto':\n",
    "    #    automatically choose the algorithm (KDTree, BallTree or Brute Force)\n",
    "    #\n",
    "    # 2. metric = 'minkowski', p = 2:\n",
    "    #    Use L2 Minkowski Distance which is nothing but Euclidean Distance.\n",
    "    #\n",
    "    # 3. n_jobs = -1: \n",
    "    #    Use all the CPU cores to apply KNN Classfication.\n",
    "    \n",
    "    # Instantiate the learning model:\n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors = k,\n",
    "        algorithm = 'auto',\n",
    "        metric = 'minkowski',\n",
    "        p = 2,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "    \n",
    "    # Fitting the model on train:\n",
    "    knn.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Predict the response on cross validation:\n",
    "    predict_y_cv = knn.predict(X_cv)\n",
    "    \n",
    "    # Evaluate the cross validation accuracy:\n",
    "    acc = accuracy_score(predict_y_cv, y_cv, normalize=True) * float(100)\n",
    "    print('\\nCross Validation Accuracy for k={} is {}%'\n",
    "         .format(k, acc))\n",
    "    \n",
    "time_taken(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Cross Validation Accuracy for k=29 is 65.18095238095239%. This is highest accuracy score out of all the accuracy scores.\n",
    "\n",
    "Therefore, we got our k=29, i.e., we will consider the majority vote of the classes of 29 nearest neighbours in the vicinity of a query point -> xq."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the K Value from Simple Cross Validation on Test Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Test Accuracy for k=29 is 64.62666666666667% *******\n"
     ]
    }
   ],
   "source": [
    "# Configured parameters are:-\n",
    "#\n",
    "# 1. algorithm = 'auto':\n",
    "#    automatically choose the algorithm (KDTree, BallTree or Brute Force)\n",
    "#\n",
    "# 2. metric = 'minkowski', p = 2:\n",
    "#    Use L2 Minkowski Distance which is nothing but Euclidean Distance.\n",
    "#\n",
    "# 3. n_jobs = -1: \n",
    "#    Use all the CPU cores to apply KNN Classfication.\n",
    "\n",
    "# Instantiate the learning model with k=29:\n",
    "k_simple = 29\n",
    "knn_simple_cv = KNeighborsClassifier(\n",
    "    n_neighbors = k_simple,\n",
    "    algorithm = 'auto',\n",
    "    metric = 'minkowski',\n",
    "    p = 2,\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "# Fitting the model on train data:\n",
    "knn_simple_cv.fit(X_tr, y_tr)\n",
    "\n",
    "# Predict the response on test data:\n",
    "predict_y_test_simple_cv = knn_simple_cv.predict(X_test)\n",
    "\n",
    "# Evaluate the test accuracy:\n",
    "acc_test_simple = accuracy_score(predict_y_test_simple_cv, y_test, normalize=True) * float(100)\n",
    "print('\\n****** Test Accuracy for k={} is {}% *******'\n",
    "     .format(k_simple, acc_test_simple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now apply K-NN using K-fold Cross Validation to get the best K, so that we can classify whether question1 is similar to question2 or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Building k-Nearest Neighbours Model using K-fold Cross Validation\n",
    "\n",
    "Here, the k used for k-NN is a Hyper Parameter which tells us the number of neighbours that the algorithm is considering before making a decision about the class of a query point.\n",
    "\n",
    "But, K used in K-fold Cross Validation is the number of folds/divisions we are making in our data, to consider the data as train and cross validation data with different division each time. After we get scores for each division, we take the mean of all of the scores, and that's our accuracy score of the k-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K = 10: 10 fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Empty list to store the cross validation scores:\n",
    "cv_scores = []\n",
    "\n",
    "st = time()\n",
    "# Perform 10-fold cross validation:\n",
    "for k in neighbours:\n",
    "        # Configured parameters are:-\n",
    "    #\n",
    "    # 1. algorithm = 'auto':\n",
    "    #    automatically choose the algorithm (KDTree, BallTree or Brute Force)\n",
    "    #\n",
    "    # 2. metric = 'minkowski', p = 2:\n",
    "    #    Use L2 Minkowski Distance which is nothing but Euclidean Distance.\n",
    "    #\n",
    "    # 3. n_jobs = -1: \n",
    "    #    Use all the CPU cores to apply KNN Classfication.\n",
    "    \n",
    "    # Instantiate the learning model:\n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors = k,\n",
    "        algorithm = 'auto',\n",
    "        metric = 'minkowski',\n",
    "        p = 2,\n",
    "        n_jobs = 3\n",
    "    )\n",
    "    \n",
    "    # cv = 10: meaning 10 folds in the given data to get combinations\n",
    "    # of train and cross validation data\n",
    "    scores = cross_val_score(\n",
    "        knn, X_train, y_train, cv=10, scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    # record all the scores until now:\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Runtime: 13361.46 seconds\n"
     ]
    }
   ],
   "source": [
    "time_taken(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.629255119617, 0.639312820543, 0.641598535053, 0.640970257035, 0.645256919435, 0.644399122845, 0.642683596016, 0.646397687043, 0.645140511216, 0.646170650141, 0.647427564818, 0.646970356804, 0.647999679272, 0.648456234785, 0.647771793651, 0.647599124002, 0.648113213611, 0.646570160102, 0.646456690733, 0.647198470493, 0.647997752536, 0.646625964463, 0.6462843481, 0.643712363698, 0.644169277965, 0.644398633366, 0.644455711029, 0.643769800489, 0.642684477891, 0.643713082066, 0.642055710246, 0.641712526273, 0.64256826544, 0.642283334754, 0.643255318745, 0.642284118502, 0.644111580174, 0.642111840951, 0.641254110114, 0.640511709816, 0.642911742898, 0.643882681916, 0.642682943103, 0.642968265664, 0.643197196333, 0.643711873884, 0.641768689762, 0.641368493545, 0.641768950726, 0.640398142208, "
     ]
    }
   ],
   "source": [
    "for i in cv_scores:\n",
    "    print(i, end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.370744880383, 0.360687179457, 0.358401464947, 0.359029742965, 0.354743080565, 0.355600877155, 0.357316403984, 0.353602312957, 0.354859488784, 0.353829349859, 0.352572435182, 0.353029643196, 0.352000320728, 0.351543765215, 0.352228206349, 0.352400875998, 0.351886786389, 0.353429839898, 0.353543309267, 0.352801529507, 0.352002247464, 0.353374035537, 0.3537156519, 0.356287636302, 0.355830722035, 0.355601366634, 0.355544288971, 0.356230199511, 0.357315522109, 0.356286917934, 0.357944289754, 0.358287473727, 0.35743173456, 0.357716665246, 0.356744681255, 0.357715881498, 0.355888419826, 0.357888159049, 0.358745889886, 0.359488290184, 0.357088257102, 0.356117318084, 0.357317056897, 0.357031734336, 0.356802803667, 0.356288126116, 0.358231310238, 0.358631506455, 0.358231049274, 0.359601857792, "
     ]
    }
   ],
   "source": [
    "# Changing to Misclassification error:\n",
    "MSE = [1-x for x in cv_scores]\n",
    "\n",
    "for i in MSE:\n",
    "    print(i, end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal number of neighbours is: 27\n"
     ]
    }
   ],
   "source": [
    "# Now, we will determine the best k:\n",
    "optimal_k = neighbours[MSE.index(min(MSE))]\n",
    "print(\"The optimal number of neighbours is:\", optimal_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAncAAAGtCAYAAAB0h1K2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5P8DUy2JVts2yNYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20065366d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Miscalssification Error for each k value is:\n",
      " [ 0.371  0.361  0.358  0.359  0.355  0.356  0.357  0.354  0.355  0.354\n",
      "  0.353  0.353  0.352  0.352  0.352  0.352  0.352  0.353  0.354  0.353\n",
      "  0.352  0.353  0.354  0.356  0.356  0.356  0.356  0.356  0.357  0.356\n",
      "  0.358  0.358  0.357  0.358  0.357  0.358  0.356  0.358  0.359  0.359\n",
      "  0.357  0.356  0.357  0.357  0.357  0.356  0.358  0.359  0.358  0.36 ]\n"
     ]
    }
   ],
   "source": [
    "# Plot the Misclassification Error v/s k:\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(neighbours, MSE)\n",
    "\n",
    "#for xy in zip(neighbours, np.round(MSE, 2)):\n",
    "    #plt.annotate('(%s, %s)' % xy, xy=xy, textcoords='data')\n",
    "\n",
    "plt.xlabel('Number of Neighbours K')\n",
    "plt.ylabel('Misclassification Error')\n",
    "plt.show()\n",
    "\n",
    "print(\"The Miscalssification Error for each k value is:\\n\", np.round(MSE, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we can see that the lowest value of Misclassification error is generated in between k=[20, 21, ..., 40].\n",
    "That's the reason, we got our optimal_k to be 27.\n",
    "\n",
    "Let us see the accuracy score after querying the k-NN model with the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Accuracy of k-NN classifier on Quora Question Pairs Dataset \n",
      "for predicting whether two given questions have the same intent or not with \n",
      "k=27 is 65.70666666666666%\n",
      "\n",
      "Runtime: 111.65 seconds\n"
     ]
    }
   ],
   "source": [
    "# KNN with k = optimal_k\n",
    "st = time()\n",
    "# Configured parameters are:-\n",
    "#\n",
    "# 1. algorithm = 'auto':\n",
    "#    automatically choose the algorithm (KDTree, BallTree or Brute Force)\n",
    "#\n",
    "# 2. metric = 'minkowski', p = 2:\n",
    "#    Use L2 Minkowski Distance which is nothing but Euclidean Distance.\n",
    "#\n",
    "# 3. n_jobs = -1: \n",
    "#    Use all the CPU cores to apply KNN Classfication.\n",
    "\n",
    "# Instantiate the learning model:\n",
    "knn_optimal = KNeighborsClassifier(\n",
    "    n_neighbors = optimal_k,\n",
    "    algorithm = 'auto',\n",
    "    metric = 'minkowski',\n",
    "    p = 2,\n",
    "    n_jobs = 3\n",
    ")\n",
    "\n",
    "# Fitting the model on train:\n",
    "knn_optimal.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response on test:\n",
    "predict_y_test = knn_optimal.predict(X_test)\n",
    "\n",
    "# Evaluate the test accuracy:\n",
    "acc_test = accuracy_score(predict_y_test, y_test, normalize=True) * float(100)\n",
    "print('''\\nThe Accuracy of k-NN classifier on Quora Question Pairs Dataset \n",
    "for predicting whether two given questions have the same intent or not with \n",
    "k={} is {}%'''.format(optimal_k, acc_test))\n",
    "\n",
    "time_taken(st)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
